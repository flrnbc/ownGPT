
# various dimensions (for attention)
d_attn = 124 # 
d_x = 32 # current token
d_z = 64 # context token
d_v = 128
d_out = 256 # output dim


# number of attention heads
attn_heads = 6

# sequence lengths 
# "features" which are in our case rows of the "attention matrices"
l_x = 16
l_z = 16

