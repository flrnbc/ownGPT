
# various dimensions (for attention)
d_attn = 1024 # 
d_x = 64 # current token
d_z = 64 # context token
d_out = 64 # output dim

# number of attention heads
H = 6

# sequence lengths 
# "features" which are in our case rows of the "attention matrices"
l_x = 16
l_z = 16

